{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmGmoP_LcrRw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7yRjogjnwOT"
      },
      "outputs": [],
      "source": [
        "#!pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3uofDPnDOp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gdown\n",
        "file_id = \"1ini0-kUwgXoVaWALnOtVWoTRmONh_8qr\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnctgJ5OlsTs"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ety1lbaRpMqM"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLlCuYhopWRG"
      },
      "outputs": [],
      "source": [
        "initial_rows = df.shape[0]\n",
        "print(f\"Rows before pre-processing = {initial_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Waq-XMbYphs2"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy_MzIsAEBhD"
      },
      "source": [
        "Pre-processing and analyzing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdbHeOgCpCRp"
      },
      "outputs": [],
      "source": [
        "df_cp = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtMmyw8zptF2"
      },
      "outputs": [],
      "source": [
        "df_cp.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzhrl7mc7k4F"
      },
      "outputs": [],
      "source": [
        "#Unique entities\n",
        "print(f\"Applications: {df['app'].nunique():,}\")\n",
        "print(f\"Functions: {df['func'].nunique():,}\")\n",
        "print(f\"App-Function pairs: {df.groupby(['app', 'func']).ngroups:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ALfpLZtqZfv"
      },
      "outputs": [],
      "source": [
        "#Converting timestamp to datetime\n",
        "df_cp['timestamp'] = pd.to_datetime(df_cp['end_timestamp'], unit = 's')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C845QQvA9B3C"
      },
      "outputs": [],
      "source": [
        "#Time range\n",
        "duration_hours = (df_cp['timestamp'].max() - df_cp['timestamp'].min()).total_seconds() / 3600\n",
        "print(f\"  Duration: {duration_hours:.1f} hours ({duration_hours/24:.1f} days)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEsgU1SorYSD"
      },
      "outputs": [],
      "source": [
        "#Removing invalid duration\n",
        "before_duration_clean = len(df_cp)\n",
        "df_cp = df_cp[(df_cp['duration'] > 0) & (df_cp['duration'] < 3600)]\n",
        "print(f\"   Removed {before_duration_clean - len(df_cp):,} rows with invalid durations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUfJcoK-r_i8"
      },
      "outputs": [],
      "source": [
        "#Checking extreme outliers\n",
        "sns.boxplot(data=df_cp, y = 'duration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC1hCGn6uUuC"
      },
      "outputs": [],
      "source": [
        "#Removing extreme outliers\n",
        "before_outliers = len(df_cp)\n",
        "Q1 = df_cp['duration'].quantile(0.25)\n",
        "Q3 = df_cp['duration'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 3* IQR\n",
        "upper_bound = Q3 + 3* IQR\n",
        "df_cp = df_cp[(df_cp['duration']>=max(0, lower_bound)) & (df_cp['duration']<=upper_bound)]\n",
        "print(f\"Removed {before_outliers - len(df_cp):,} extreme outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-pwGMGu2O_1"
      },
      "outputs": [],
      "source": [
        "#Sorting based on timestamp\n",
        "df_cp = df_cp.sort_values('timestamp').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfSieLpw3JSf"
      },
      "outputs": [],
      "source": [
        "processed_rows = df_cp.shape[0]\n",
        "print(f\"Length of processed dataset ={processed_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkezX6e23Lo3"
      },
      "outputs": [],
      "source": [
        "#Data Retention Rate\n",
        "print(f\"Data retention rate = {processed_rows/initial_rows *100:.2f}% \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmF1n3GO2n4i"
      },
      "source": [
        "Feature Engineering & Analysing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_ogY_xL49TD"
      },
      "outputs": [],
      "source": [
        "df_cp['hour'] = df_cp['timestamp'].dt.hour\n",
        "df_cp['dayofweek'] = df_cp['timestamp'].dt.dayofweek\n",
        "df_cp['dayofmonth'] = df_cp['timestamp'].dt.day\n",
        "df_cp['month'] = df_cp['timestamp'].dt.month\n",
        "df_cp['is_weekend'] = df_cp['dayofweek'].isin([5,6]).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUQIJepJ5rYC"
      },
      "outputs": [],
      "source": [
        "#Identifying cold starts\n",
        "df_cp = df_cp.sort_values(['app', 'func','timestamp'])\n",
        "df_cp['time_gap'] = df_cp.groupby(['app','func'])['timestamp'].diff().dt.total_seconds()\n",
        "cold_start_threshold = 600\n",
        "df_cp['is_potential_coldstart'] = ((df_cp['time_gap']>cold_start_threshold) | (df_cp['time_gap'].isna())).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXzLf2qwUFrh"
      },
      "outputs": [],
      "source": [
        "#Function STatistics Analysis\n",
        "\n",
        "func_stats = df_cp.groupby(['app', 'func']).agg({\n",
        "    'duration':['count', 'mean', 'std', 'min', 'max'],\n",
        "    'is_potential_coldstart':['sum', 'mean'],\n",
        "    'time_gap': ['mean'],\n",
        "    'timestamp':['min','max']\n",
        "}).reset_index()\n",
        "\n",
        "\n",
        "# Flatten column names\n",
        "func_stats.columns = [\n",
        "        'app', 'func', 'invocation_count', 'avg_duration', 'std_duration',\n",
        "        'min_duration', 'max_duration', 'cold_starts', 'cold_start_rate',\n",
        "        'avg_time_gap', 'first_invocation', 'last_invocation'\n",
        "]\n",
        "\n",
        "func_stats['activity_span_hours'] = (func_stats['last_invocation'] - func_stats['first_invocation']).dt.total_seconds()/3600\n",
        "func_stats['invocations_per_hour'] = func_stats['invocation_count']/np.maximum(func_stats['activity_span_hours'],0.1)\n",
        "func_stats['duration_variability'] = func_stats['std_duration']/np.maximum(func_stats['avg_duration'], 0.001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXBm3HUEk1Yy"
      },
      "outputs": [],
      "source": [
        "func_stats = func_stats.sort_values('invocation_count', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G3GyazNlWKy"
      },
      "outputs": [],
      "source": [
        "print(f\"Total unique functions: {len(func_stats):,}\")\n",
        "print(f\"Functions with >1000 invocations: {(func_stats['invocation_count'] > 1000).sum()}\")\n",
        "print(f\"Functions with >100 invocations: {(func_stats['invocation_count'] > 100).sum()}\")\n",
        "print(f\"Functions with >10 invocations: {(func_stats['invocation_count'] > 10).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6Y5q-mYlmFx"
      },
      "outputs": [],
      "source": [
        "#Top 10 most active functions:\n",
        "for i, (_, row) in enumerate(func_stats.head(10).iterrows()):\n",
        "  print(f\"\\n{i+1}. Function: {row['func']}\")\n",
        "  print(f\"Invocations: {row['invocation_count']:,}\")\n",
        "  print(f\"Avg duration: {row['avg_duration']:.3f}s\")\n",
        "  print(f\"Cold start rate: {row['cold_start_rate']*100:.1f}%\")\n",
        "  print(f\"Activity span: {row['activity_span_hours']:.1f} hours\")\n",
        "  print(f\"Invocations/hour: {row['invocations_per_hour']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_EOZjYkA3GX"
      },
      "outputs": [],
      "source": [
        "#Invocation statistics\n",
        "print(f\"Total invocations = {len(df_cp):,}\")\n",
        "print(f\"Total invocations per hour = {len(df_cp)/duration_hours:.0f}\")\n",
        "print(f\"Total invocations per function = {len(df_cp)/df_cp['func'].nunique():.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WI4l1OyFNs6"
      },
      "outputs": [],
      "source": [
        "#Duration Analysis\n",
        "duration_stats = df_cp['duration'].describe()\n",
        "print(\"Duration statistics (seconds):\")\n",
        "for stat, value in duration_stats.items():\n",
        "  print(f\"{stat} : {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18fieDs3GgjY"
      },
      "outputs": [],
      "source": [
        "#Duration Percentile\n",
        "percentiles = [90,95,99,99.9]\n",
        "for p in percentiles:\n",
        "  value = df_cp['duration'].quantile(p/100)\n",
        "  print(f\"{p}th percentile: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1dgkiH9GLDW"
      },
      "outputs": [],
      "source": [
        "#Duration Categories\n",
        "short_duration = (df_cp['duration']<=0.1).sum()\n",
        "medium_duration = ((df_cp['duration']>0.1) & (df_cp['duration']<=1.0)).sum()\n",
        "long_duration = (df_cp['duration']>1.0).sum()\n",
        "print(f\"Very fast (<0.1s) = {short_duration:,} ({short_duration/len(df_cp)*100:.1f}%)\")\n",
        "print(f\"Medium (0.1s - 1.0s) = {medium_duration:,} ({medium_duration/len(df_cp)*100:.1f}%)\")\n",
        "print(f\"Slow (>1.0s) = {long_duration:,} ({long_duration/len(df_cp)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmJxQIwcXspk"
      },
      "outputs": [],
      "source": [
        "#Cold start analysis\n",
        "total_invocations = len(df_cp)\n",
        "cold_starts = df_cp['is_potential_coldstart'].sum()\n",
        "cold_start_rate = cold_starts/total_invocations *100\n",
        "print(f\"Total invocations: {total_invocations:,}\")\n",
        "print(f\"Potential cold starts: {cold_starts:,}\")\n",
        "print(f\"Cold start rate: {cold_start_rate:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppVT60WMMI1h"
      },
      "outputs": [],
      "source": [
        "# Cold start by time gap analysis\n",
        "time_gaps = df_cp[df_cp['time_gap'].notna()]['time_gap']\n",
        "gap_stats = time_gaps.describe()\n",
        "for stat, value in gap_stats.items():\n",
        "  if stat in ['mean', 'std', 'min', 'max']:\n",
        "    print(f\"{stat} : {value:,.0f} seconds ({value/60:.1f} minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Et0w5cTO3l9"
      },
      "outputs": [],
      "source": [
        "# Cold start vs warm start duration comparison\n",
        "cold_duration = df_cp[df_cp['is_potential_coldstart'] == 1]['duration'].mean()\n",
        "warm_duration = df_cp[df_cp['is_potential_coldstart'] == 0]['duration'].mean()\n",
        "\n",
        "print(f\"Cold start avg duration: {cold_duration:.4f}s\")\n",
        "print(f\"Warm start avg duration: {warm_duration:.4f}s\")\n",
        "print(f\"Cold start overhead: {cold_duration - warm_duration:.4f}s ({(cold_duration/warm_duration-1)*100:.1f}% increase)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcbVsd7hPTmO"
      },
      "outputs": [],
      "source": [
        "df_cp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLGGwTejZqnU"
      },
      "outputs": [],
      "source": [
        "#Pattern Analysis\n",
        "hourly_stats = df_cp.groupby('hour').agg({\n",
        "    'func':'count',\n",
        "    'duration': 'mean',\n",
        "    'is_potential_coldstart':['sum','mean']\n",
        "})\n",
        "hourly_stats.columns = ['invocations', 'avg_duration', 'cold_starts', 'cold_start_rate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8gVqK_t6PrV"
      },
      "outputs": [],
      "source": [
        "hourly_stats.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEePDrKm6x8t"
      },
      "outputs": [],
      "source": [
        "#Hourly patterns\n",
        "peak_hour = hourly_stats['invocations'].idxmax()\n",
        "quiet_hour = hourly_stats['invocations'].idxmin()\n",
        "peak_coldstart_hour = hourly_stats['cold_start_rate'].idxmax()\n",
        "\n",
        "print(f\"Peak hour: {peak_hour}:00 ({hourly_stats.loc[peak_hour, 'invocations']:,} invocations)\")\n",
        "print(f\"Quietest hour: {quiet_hour}:00 ({hourly_stats.loc[quiet_hour, 'invocations']:,} invocations)\")\n",
        "print(f\"Cold Start Hour: {peak_coldstart_hour}:00 ({hourly_stats.loc[peak_coldstart_hour, 'cold_start_rate']*100:.1f}% invocations)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grS4ffqZB776"
      },
      "outputs": [],
      "source": [
        "#Daily Patterns\n",
        "daily_stats = df_cp.groupby('dayofweek').agg({\n",
        "    'func' : 'count',\n",
        "    'duration' : 'mean',\n",
        "    'is_potential_coldstart' : ['sum', 'mean']\n",
        "})\n",
        "daily_stats.columns = ['invocations', 'avg_duration','cold_starts', 'cold_start_rate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCC43TkYEuIC"
      },
      "outputs": [],
      "source": [
        "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "for i, day in enumerate(days):\n",
        "  if i in daily_stats.index:\n",
        "    invocations = daily_stats.loc[i, 'invocations']\n",
        "    coldstart_rate = daily_stats.loc[i, 'cold_start_rate'] * 100\n",
        "    print(f\"{day}: {invocations:,} invocations, {coldstart_rate:.1f}% cold starts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OORsM0ArFEaR"
      },
      "outputs": [],
      "source": [
        "# Weekend vs Weekday\n",
        "weekend_comparison = df_cp.groupby('is_weekend').agg({\n",
        "        'func': 'count',\n",
        "        'duration': 'mean',\n",
        "        'is_potential_coldstart': 'mean'\n",
        "})\n",
        "weekend_comparison.columns = ['invocations', 'avg_duration', 'cold_start_rate']\n",
        "\n",
        "print(f\"\\nWeekend vs Weekday comparison:\")\n",
        "print(f\"Weekdays: {weekend_comparison.loc[0, 'invocations']:,} invocations, {weekend_comparison.loc[0, 'cold_start_rate']*100:.1f}% cold starts\")\n",
        "print(f\"Weekends: {weekend_comparison.loc[1, 'invocations']:,} invocations, {weekend_comparison.loc[1, 'cold_start_rate']*100:.1f}% cold starts\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYE8LE-evRaL"
      },
      "source": [
        "Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9AyMuEhtm0j"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Azure Functions Dataset Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "#hourly traffic pattern(plot 1)\n",
        "axes[0, 0].plot(hourly_stats.index, hourly_stats['invocations'],\n",
        "                marker='o', linewidth=2, markersize=6, color='blue')\n",
        "axes[0, 0].set_title('Traffic by Hour of Day', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Hour')\n",
        "axes[0, 0].set_ylabel('Number of Invocations')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_xticks(range(0, 24, 4))\n",
        "\n",
        "# Daily traffic pattern (Plot 2)\n",
        "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "axes[0, 1].bar(range(len(daily_stats)), daily_stats['invocations'],\n",
        "               color='skyblue', alpha=0.8)\n",
        "axes[0, 1].set_title('Traffic by Day of Week', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Day')\n",
        "axes[0, 1].set_ylabel('Number of Invocations')\n",
        "axes[0, 1].set_xticks(range(len(days)))\n",
        "axes[0, 1].set_xticklabels(days)\n",
        "\n",
        "#Duration distribution\n",
        "axes[0, 2].hist(df['duration'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[0, 2].set_title('Function Duration Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Duration (seconds)')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_yscale('log')\n",
        "\n",
        "#Cold start rate by hour\n",
        "axes[1, 0].plot(hourly_stats.index, hourly_stats['cold_start_rate'] * 100,\n",
        "           marker='s', color='red', linewidth=2, markersize=5)\n",
        "axes[1, 0].set_title('Cold Start Rate by Hour', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Hour')\n",
        "axes[1, 0].set_ylabel('Cold Start Rate (%)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_xticks(range(0, 24, 4))\n",
        "\n",
        "#Top functions by invocations\n",
        "top_funcs = df['func'].value_counts().head(10)\n",
        "axes[1, 1].barh(range(len(top_funcs)), top_funcs.values,\n",
        "                color='orange', alpha=0.7)\n",
        "axes[1, 1].set_title('Top 10 Functions by Invocation Count', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Number of Invocations')\n",
        "axes[1, 1].set_yticks(range(len(top_funcs)))\n",
        "axes[1, 1].set_yticklabels([f\"...{func[-12:]}\" for func in top_funcs.index])\n",
        "\n",
        "#Cold start vs warm start duration comparison\n",
        "cold_durations = df_cp[df_cp['is_potential_coldstart'] == 1]['duration']\n",
        "warm_durations = df_cp[df_cp['is_potential_coldstart'] == 0]['duration']\n",
        "\n",
        "axes[1, 2].hist([warm_durations, cold_durations], bins=30, alpha=0.7,\n",
        "                    label=['Warm Start', 'Cold Start'], color=['blue', 'red'])\n",
        "axes[1, 2].set_title('Duration: Cold Start vs Warm Start', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Duration (seconds)')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvGdNNPSvhAI"
      },
      "outputs": [],
      "source": [
        "#Time gap distribution analysis\n",
        "time_gaps = df_cp[df_cp['time_gap'].notna()]['time_gap'] #first invocation of each function\n",
        "\n",
        "print(f\"  Total gaps analyzed: {len(time_gaps):,}\")\n",
        "print(f\"  Mean gap: {time_gaps.mean():.0f} seconds ({time_gaps.mean()/60:.1f} minutes)\")\n",
        "print(f\"  Median gap: {time_gaps.median():.0f} seconds ({time_gaps.median()/60:.1f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePNtFljQvr4w"
      },
      "outputs": [],
      "source": [
        "# Gap categories\n",
        "categories = {\n",
        "        'Very short (≤10s)': (time_gaps <= 10).sum(),\n",
        "        'Short (10s-1min)': ((time_gaps > 10) & (time_gaps <= 60)).sum(),\n",
        "        'Medium (1-10min)': ((time_gaps > 60) & (time_gaps <= 600)).sum(),\n",
        "        'Long (10min-1hr)': ((time_gaps > 600) & (time_gaps <= 3600)).sum(),\n",
        "        'Very long (>1hr)': (time_gaps > 3600).sum()\n",
        "}\n",
        "\n",
        "for category, count in categories.items():\n",
        "  percentage = (count / len(time_gaps)) * 100\n",
        "  print(f\"  {category}: {count:,} ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSkX7LGKyxBs"
      },
      "outputs": [],
      "source": [
        "#time gap distribution\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(time_gaps, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "plt.xlabel('Time Gap (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Time Gap Distribution (All)')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Focusing on gaps up to 1 hour for better visibility\n",
        "short_gaps = time_gaps[time_gaps <= 3600]\n",
        "plt.hist(short_gaps, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "plt.xlabel('Time Gap (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Time Gap Distribution (≤1 hour)')\n",
        "plt.axvline(x=600, color='red', linestyle='--', linewidth=2, label='Cold Start Threshold')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ErY7zryy-aK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter functions with reasonable activity (>50 invocations)\n",
        "active_funcs = func_stats[func_stats['invocation_count'] >= 50].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKU6LQe5zddu"
      },
      "outputs": [],
      "source": [
        "active_funcs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHD44A8CzhI5"
      },
      "outputs": [],
      "source": [
        "active_funcs['regularity_score'] = 1 / (1 + active_funcs['duration_variability'])  # Higher = more regular\n",
        "active_funcs['activity_intensity'] = active_funcs['invocations_per_hour']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELkp98uHD20v"
      },
      "outputs": [],
      "source": [
        "active_funcs.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWObToQUzq7J"
      },
      "outputs": [],
      "source": [
        "# Classifying workloads\n",
        "conditions = [\n",
        "        (active_funcs['cold_start_rate'] > 0.3) & (active_funcs['activity_intensity'] < 15),       # Sporadic (High cold start, Low intensity)\n",
        "        (active_funcs['cold_start_rate'] < 0.1) & (active_funcs['activity_intensity'] > 1000),      # Steady High (Low cold start, Very High intensity)\n",
        "        (active_funcs['activity_intensity'] > 30) & (active_funcs['regularity_score'] > 0.5),      # Regular (Moderate/High intensity AND predictable)\n",
        "        (active_funcs['cold_start_rate'] > 0.15) & (active_funcs['duration_variability'] > 2),     # Bursty (Medium cold start, High variability)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIJC8yHdzyUD"
      },
      "outputs": [],
      "source": [
        "choices = ['Sporadic', 'Steady-High', 'Regular', 'Bursty']\n",
        "active_funcs['workload_type'] = np.select(conditions, choices, default='Mixed')\n",
        "\n",
        "workload_summary = active_funcs['workload_type'].value_counts()\n",
        "print(f\"Workload type distribution (functions with ≥50 invocations):\")\n",
        "for workload_type, count in workload_summary.items():\n",
        "  percentage = (count / len(active_funcs)) * 100\n",
        "  print(f\"  {workload_type}: {count} functions ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL6kNNyqz9Pf"
      },
      "outputs": [],
      "source": [
        "#workload type example\n",
        "for workload_type in ['Sporadic', 'Steady-High', 'Regular', 'Bursty', 'Mixed']:\n",
        "  if workload_type in active_funcs['workload_type'].values:\n",
        "    example = active_funcs[active_funcs['workload_type'] == workload_type].iloc[0]\n",
        "    print(f\"\\n{workload_type} example:\")\n",
        "    print(f\"Function: {example['func'][:20]}...\")\n",
        "    print(f\"Invocations: {example['invocation_count']:,}\")\n",
        "    print(f\"Cold start rate: {example['cold_start_rate']*100:.1f}%\")\n",
        "    print(f\"Invocations/hour: {example['activity_intensity']:.1f}\")\n",
        "    print(f\"Duration variability: {example['duration_variability']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKRhWNCH0Gtm"
      },
      "outputs": [],
      "source": [
        "df_features = df_cp.sort_values(['app', 'func', 'timestamp'])\n",
        "\n",
        "print(\"Creating cyclical time features...\")\n",
        "# Cyclical encoding for time features (better for ML models)\n",
        "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\n",
        "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\n",
        "df_features['dayofweek_sin'] = np.sin(2 * np.pi * df_features['dayofweek'] / 7)\n",
        "df_features['dayofweek_cos'] = np.cos(2 * np.pi * df_features['dayofweek'] / 7)\n",
        "df_features['dayofmonth_sin'] = np.sin(2 * np.pi * df_features['dayofmonth'] / 31)\n",
        "df_features['dayofmonth_cos'] = np.cos(2 * np.pi * df_features['dayofmonth'] / 31)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzA6ssPb0046"
      },
      "outputs": [],
      "source": [
        "# Business hours and weekend patterns\n",
        "df_features['is_business_hours'] = ((df_features['hour'] >= 9) &\n",
        "                                      (df_features['hour'] <= 17) &\n",
        "                                       (df_features['is_weekend'] == 0)).astype(int)\n",
        "df_features['is_peak_hours'] = ((df_features['hour'] >= 10) &\n",
        "                                   (df_features['hour'] <= 15)).astype(int)\n",
        "df_features['is_evening'] = ((df_features['hour'] >= 18) &\n",
        "                                (df_features['hour'] <= 23)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJAGd-aJ0-_p"
      },
      "outputs": [],
      "source": [
        "df_features = df_features.sort_values(by=['app', 'func', 'timestamp'])\n",
        "\n",
        "# Calculate Rolling Features\n",
        "\n",
        "windows = [5, 15, 30]\n",
        "for window in windows:\n",
        "  # Rolling mean of durations\n",
        "  df_features[f'duration_rolling_mean_{window}'] = (\n",
        "      df_features.groupby(['app', 'func'])['duration']\n",
        "      .rolling(window=window, min_periods=1)\n",
        "      .mean()\n",
        "      .reset_index(level=[0, 1], drop=True) #to drop the new app and func levels created by groupby\n",
        "  )\n",
        "\n",
        "  # Rolling std of durations\n",
        "  df_features[f'duration_rolling_std_{window}'] = (\n",
        "      df_features.groupby(['app', 'func'])['duration']\n",
        "      .rolling(window=window, min_periods=1)\n",
        "      .std()\n",
        "      .fillna(0)\n",
        "      .reset_index(level=[0, 1], drop=True)\n",
        "  )\n",
        "\n",
        "  # Rolling count (frequency)\n",
        "  df_features[f'frequency_{window}'] = (\n",
        "      df_features.groupby(['app', 'func'])['duration']\n",
        "      .rolling(window=window, min_periods=1)\n",
        "      .count()\n",
        "      .reset_index(level=[0, 1], drop=True)\n",
        "  )\n",
        "\n",
        "print(\"Creating lag features...\")\n",
        "#Lag features\n",
        "lags = [1, 2, 3, 5]\n",
        "for lag in lags:\n",
        "    df_features[f'duration_lag_{lag}'] = (\n",
        "        df_features.groupby(['app', 'func'])['duration']\n",
        "        .shift(lag)\n",
        "        .fillna(df_features['duration'].mean())\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgV_HyK11NiO"
      },
      "outputs": [],
      "source": [
        "# Inter-arrival time features\n",
        "df_features['time_since_last'] = df_features.groupby(['app', 'func'])['timestamp'].diff().dt.total_seconds().fillna(0)\n",
        "df_features['time_since_last_log'] = np.log1p(df_features['time_since_last'])\n",
        "\n",
        "# Inverse frequency (how rare are invocations for this function)\n",
        "df_features['inv_frequency'] = 1.0 / (df_features.groupby(['app', 'func']).cumcount() + 1)\n",
        "\n",
        "print(\"Creating burst detection features...\")\n",
        "# Burst detection\n",
        "df_features['is_burst'] = (df_features['time_since_last'] < 10).astype(int)  # Calls within 10 seconds\n",
        "df_features['burst_size'] = df_features.groupby(['app', 'func'])['is_burst'].rolling(10, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqZLoQFu1zLu"
      },
      "outputs": [],
      "source": [
        "df_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgfG7S-JIPH0"
      },
      "outputs": [],
      "source": [
        "df_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dHJvjx9IxfK"
      },
      "source": [
        "Feature Corelation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvw1WahJIR9k"
      },
      "outputs": [],
      "source": [
        "# Selecting numeric columns for correlation analysis\n",
        "numeric_cols = df_features.select_dtypes(include = [np.number]).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwO1VH4gJWbS"
      },
      "outputs": [],
      "source": [
        "# Removing identifier columns\n",
        "feature_cols = [col for col in numeric_cols\n",
        "                   if col not in ['app', 'func'] and 'timestamp' not in col.lower()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYBVDGT_Jv2s"
      },
      "outputs": [],
      "source": [
        "# Computing correlation matrix\n",
        "correlation_matrix = df_features[feature_cols].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzjLdcorJ4nC"
      },
      "outputs": [],
      "source": [
        "# Finding highly correlated feature pairs\n",
        "high_corr_pairs = []\n",
        "# Looping through the upper triangle of the matrix to find pairs\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "  for j in range(i + 1, len(correlation_matrix.columns)):\n",
        "\n",
        "    corr_val = correlation_matrix.iloc[i, j]\n",
        "    if abs(corr_val) > 0.7:  # High correlation threshold\n",
        "      high_corr_pairs.append((\n",
        "      correlation_matrix.columns[i],\n",
        "      correlation_matrix.columns[j],\n",
        "      corr_val\n",
        "      ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDJhaFJLKFo6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Visualize correlation heatmap for key features\n",
        "key_features = [\n",
        "        'duration', 'is_potential_coldstart', 'time_gap', 'hour', 'dayofweek',\n",
        "        'is_business_hours', 'duration_rolling_mean_15', 'frequency_15',\n",
        "        'time_since_last_log', 'burst_size'\n",
        "]\n",
        "\n",
        "available_features = [f for f in key_features if f in df_features.columns]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df_features[available_features].corr(),\n",
        "                annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f')\n",
        "plt.title('Feature Correlation Matrix (Key Features)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UQLvdJUL7EV"
      },
      "source": [
        "Creating Time Series Dataset for Top Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K5r9QTILkpi"
      },
      "outputs": [],
      "source": [
        "# Selecting top functions with sufficient data\n",
        "top_functions = func_stats.head(10)\n",
        "time_series_datasets = {}\n",
        "\n",
        "for idx, row in top_functions.iterrows():\n",
        "  app_id = row['app']\n",
        "  func_id = row['func']\n",
        "\n",
        "  print(f\"\\nProcessing function {idx+1}/10: {func_id}...\")\n",
        "  print(f\"  Total invocations: {row['invocation_count']:,}\")\n",
        "\n",
        "  # Get data for this function\n",
        "  func_data = df_features[\n",
        "  (df_features['app'] == app_id) &\n",
        "  (df_features['func'] == func_id)\n",
        "  ].copy()\n",
        "\n",
        "  # Create minute-level time series\n",
        "  func_data['timestamp_minute'] = func_data['timestamp'].dt.floor('1min')\n",
        "\n",
        "  # Aggregate by minute\n",
        "  minute_series = func_data.groupby('timestamp_minute').agg({\n",
        "           'duration': ['count', 'mean', 'std'],\n",
        "            'is_potential_coldstart': 'sum',\n",
        "            'hour': 'first',\n",
        "            'dayofweek': 'first',\n",
        "            'is_business_hours': 'first',\n",
        "            'is_weekend': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "  # Flatten column names\n",
        "  minute_series.columns = [\n",
        "            'ds', 'y', 'avg_duration', 'std_duration',\n",
        "            'cold_starts', 'hour', 'dayofweek',\n",
        "            'is_business_hours', 'is_weekend'\n",
        "        ]\n",
        "\n",
        "  # Fill missing minutes with zeros\n",
        "  start_time = minute_series['ds'].min()\n",
        "  end_time = minute_series['ds'].max()\n",
        "  complete_range = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
        "\n",
        "  minute_series = minute_series.set_index('ds').reindex(complete_range, fill_value=0)\n",
        "  minute_series.index.name = 'ds'\n",
        "  minute_series = minute_series.reset_index()\n",
        "\n",
        "  # Forward fill categorical features\n",
        "  minute_series['hour'] = minute_series['ds'].dt.hour\n",
        "  minute_series['dayofweek'] = minute_series['ds'].dt.dayofweek\n",
        "  minute_series['is_business_hours'] = ((minute_series['hour'] >= 9) &\n",
        "                                             (minute_series['hour'] <= 17) &\n",
        "                                             (minute_series['dayofweek'] < 5)).astype(int)\n",
        "  minute_series['is_weekend'] = minute_series['dayofweek'].isin([5, 6]).astype(int)\n",
        "\n",
        "  # Store dataset\n",
        "  dataset_info = {\n",
        "            'data': minute_series,\n",
        "            'function_id': func_id + '...',\n",
        "            'app_id': app_id + '...',\n",
        "            'total_invocations': row['invocation_count'],\n",
        "            'data_points': len(minute_series),\n",
        "            'activity_hours': row['activity_span_hours'],\n",
        "            'cold_start_rate': row['cold_start_rate']\n",
        "        }\n",
        "\n",
        "  time_series_datasets[f'func_{idx+1}'] = dataset_info\n",
        "\n",
        "  print(f\"Time series length: {len(minute_series):,} minutes\")\n",
        "  print(f\"Non-zero periods: {(minute_series['y'] > 0).sum():,}\")\n",
        "  print(f\"Coverage: {(minute_series['y'] > 0).mean()*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1J819HNNBJi"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\n Created time series for {len(time_series_datasets)} functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlSP_k84OSyW"
      },
      "source": [
        "Export Data for ML Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3EVq4JWNw-p"
      },
      "outputs": [],
      "source": [
        "# Export full processed dataset\n",
        "print(\"Exporting full processed dataset\")\n",
        "df_features.to_csv('azure_functions_processed_full.csv', index=False)\n",
        "print(f\"   Saved: azure_functions_processed_full.csv ({df_features.shape[0]:,} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYkNnattOgrN"
      },
      "outputs": [],
      "source": [
        "# Export individual function time series for Prophet/LSTM\n",
        "print(\"Exporting individual function time series...\")\n",
        "for func_name, dataset in time_series_datasets.items():\n",
        "  filename = f'timeseries_{func_name}.csv'\n",
        "  dataset['data'].to_csv(filename, index=False)\n",
        "  print(f\"   Saved: {filename} ({len(dataset['data']):,} data points)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKupTNQCPELs"
      },
      "outputs": [],
      "source": [
        "# Export summary statistics\n",
        "print(\"Creating summary report...\")\n",
        "summary_data = []\n",
        "for func_name, dataset in time_series_datasets.items():\n",
        "  data = dataset['data']\n",
        "  summary_data.append({\n",
        "            'function_name': func_name,\n",
        "            'function_id': dataset['function_id'],\n",
        "            'total_invocations': dataset['total_invocations'],\n",
        "            'time_series_length': len(data),\n",
        "            'non_zero_periods': (data['y'] > 0).sum(),\n",
        "            'coverage_pct': (data['y'] > 0).mean() * 100,\n",
        "            'max_requests_per_minute': data['y'].max(),\n",
        "            'avg_requests_per_minute': data['y'].mean(),\n",
        "            'cold_start_rate_pct': dataset['cold_start_rate'] * 100\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPfJsoJBPJ27"
      },
      "outputs": [],
      "source": [
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df.to_csv('dataset_summary.csv', index=False)\n",
        "print(f\"   Saved: dataset_summary.csv\")\n",
        "\n",
        "print(f\"\\n Data export completed!\")\n",
        "print(f\"Ready for Prophet/LSTM model training on {len(time_series_datasets)} functions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM5N8_OmPZSv"
      },
      "outputs": [],
      "source": [
        "#Final Analysis Summary\n",
        "print(\"Dataset Processing Summary:\")\n",
        "print(f\"Original dataset: {len(df):,} invocations\")\n",
        "print(f\"Processed dataset: {len(df_features):,} invocations\")\n",
        "print(f\"Features created: {df_features.shape[1]} columns\")\n",
        "print(f\"Time series functions: {len(time_series_datasets)}\")\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(f\"Total functions analyzed: {df_features['func'].nunique():,}\")\n",
        "print(f\"Average cold start rate: {df_features['is_potential_coldstart'].mean()*100:.1f}%\")\n",
        "print(f\"Peak traffic hour: {df_features.groupby('hour').size().idxmax()}:00\")\n",
        "print(f\"Most active day: {['Mon','Tue','Wed','Thu','Fri','Sat','Sun'][df_features.groupby('dayofweek').size().idxmax()]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3sL_bXMP3b5"
      },
      "outputs": [],
      "source": [
        "df_features.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8WBINP5IWtO"
      },
      "source": [
        "Prophet Model Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp_15351HqX1"
      },
      "outputs": [],
      "source": [
        "df_243 = pd.read_csv('timeseries_func_243.csv')\n",
        "print(df_243['y'].describe())\n",
        "print(f\"Zero traffic periods: {(df_243['y'] == 0).sum() / len(df_243) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GKe-dVhXhGD"
      },
      "outputs": [],
      "source": [
        "df_101 = pd.read_csv('timeseries_func_101.csv')\n",
        "print(df_101['y'].describe())\n",
        "print(f\"Zero traffic periods: {(df_101['y'] == 0).sum() / len(df_101) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pnFVAKdMoKr"
      },
      "outputs": [],
      "source": [
        "FUNCTION_IDS = [\n",
        "    'func_235', 'func_242', 'func_126', 'func_186',\n",
        "     'func_190', 'func_110', 'func_99', 'func_191'\n",
        "] #'func_243' and 'func_101' are excluded because of High zero-rate (80.9%) - reactive-only\n",
        "\n",
        "TRAIN_RATIO = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JisxjkLxIusB"
      },
      "outputs": [],
      "source": [
        "def train_prophet_model(df, func_id):\n",
        "\n",
        "    # Fix data types\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
        "\n",
        "    # Remove any NaN values\n",
        "    df = df.dropna(subset=['ds', 'y'])\n",
        "\n",
        "    # Train-test split\n",
        "    split_idx = int(len(df) * TRAIN_RATIO)\n",
        "    train = df.iloc[:split_idx][['ds', 'y']].copy()\n",
        "    test = df.iloc[split_idx:][['ds', 'y']].copy()\n",
        "\n",
        "    # Configure model\n",
        "    model = Prophet(\n",
        "        changepoint_prior_scale=0.05,\n",
        "        seasonality_prior_scale=10.0,\n",
        "        seasonality_mode='multiplicative',\n",
        "        daily_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=False,\n",
        "        interval_width=0.95\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    model.fit(train)\n",
        "\n",
        "    # Predict\n",
        "    forecast = model.predict(test[['ds']])\n",
        "\n",
        "    # Calculate metrics\n",
        "    y_true = test['y'].values\n",
        "    y_pred = np.maximum(forecast['yhat'].values, 0)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1))) * 100\n",
        "\n",
        "    # Detect anomalies\n",
        "    forecast_test = forecast.set_index('ds')\n",
        "    test_indexed = test.set_index('ds')\n",
        "    anomalies = test_indexed[\n",
        "        (test_indexed['y'] < forecast_test['yhat_lower']) |\n",
        "        (test_indexed['y'] > forecast_test['yhat_upper'])\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        'function': func_id,\n",
        "        'train_size': len(train),\n",
        "        'test_size': len(test),\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'mape': mape,\n",
        "        'anomalies': len(anomalies),\n",
        "        'anomaly_rate': len(anomalies) / len(test) * 100,\n",
        "        'model': model,\n",
        "        'forecast': forecast,\n",
        "        'test': test\n",
        "    }\n",
        "\n",
        "print(\"Starting batch training for all functions...\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for func_id in tqdm(FUNCTION_IDS, desc=\"Training models\"):\n",
        "    try:\n",
        "        # Load data with proper type handling\n",
        "        df = pd.read_csv(f'timeseries_{func_id}.csv')\n",
        "\n",
        "        # Convert data types explicitly\n",
        "        df['ds'] = pd.to_datetime(df['ds'])\n",
        "        df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
        "\n",
        "        # Handle boolean columns if they exist\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Try to convert string booleans to numeric\n",
        "                if df[col].str.lower().isin(['true', 'false']).any():\n",
        "                    df[col] = df[col].str.lower().map({'true': 1, 'false': 0})\n",
        "\n",
        "        # Drop any rows with NaN in critical columns\n",
        "        df = df.dropna(subset=['ds', 'y'])\n",
        "\n",
        "        if len(df) == 0:\n",
        "            raise ValueError(\"No valid data after cleaning\")\n",
        "\n",
        "        # Train model\n",
        "        result = train_prophet_model(df, func_id)\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"\\n {func_id}: MAE={result['mae']:.2f}, RMSE={result['rmse']:.2f}, MAPE={result['mape']:.1f}%\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n {func_id}: File not found - timeseries_{func_id}.csv\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\n {func_id}: Data Issue - {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n {func_id}: Error - {str(e)}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Completed: {len(results)}/{len(FUNCTION_IDS)} functions\")\n",
        "\n",
        "# COMPARE RESULTS\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame([{\n",
        "    'function': r['function'],\n",
        "    'mae': r['mae'],\n",
        "    'rmse': r['rmse'],\n",
        "    'mape': r['mape'],\n",
        "    'anomaly_rate': r['anomaly_rate']\n",
        "} for r in results])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(f\"  MAE:  {summary_df['mae'].mean():.2f}\")\n",
        "print(f\"  RMSE: {summary_df['rmse'].mean():.2f}\")\n",
        "print(f\"  MAPE: {summary_df['mape'].mean():.1f}%\")\n",
        "\n",
        "# Save summary\n",
        "summary_df.to_csv('prophet_comparison_results.csv', index=False)\n",
        "\n",
        "# VISUALIZE COMPARISON\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: MAE comparison\n",
        "axes[0, 0].barh(summary_df['function'], summary_df['mae'], color='steelblue')\n",
        "axes[0, 0].set_xlabel('Mean Absolute Error')\n",
        "axes[0, 0].set_title('MAE by Function')\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2: MAPE comparison\n",
        "axes[0, 1].barh(summary_df['function'], summary_df['mape'], color='coral')\n",
        "axes[0, 1].set_xlabel('Mean Absolute Percentage Error (%)')\n",
        "axes[0, 1].set_title('MAPE by Function')\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 3: Anomaly rate\n",
        "axes[1, 0].barh(summary_df['function'], summary_df['anomaly_rate'], color='indianred')\n",
        "axes[1, 0].set_xlabel('Anomaly Rate (%)')\n",
        "axes[1, 0].set_title('Anomaly Detection Rate')\n",
        "axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 4: MAE vs MAPE scatter\n",
        "axes[1, 1].scatter(summary_df['mae'], summary_df['mape'], s=100, alpha=0.6, color='purple')\n",
        "for idx, row in summary_df.iterrows():\n",
        "    axes[1, 1].annotate(row['function'].split('_')[1],\n",
        "                        (row['mae'], row['mape']),\n",
        "                        fontsize=8, alpha=0.7)\n",
        "axes[1, 1].set_xlabel('MAE')\n",
        "axes[1, 1].set_ylabel('MAPE (%)')\n",
        "axes[1, 1].set_title('MAE vs MAPE')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prophet_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# IDENTIFY BEST/WORST PERFORMERS\n",
        "\n",
        "best_mae = summary_df.loc[summary_df['mae'].idxmin()]\n",
        "worst_mae = summary_df.loc[summary_df['mae'].idxmax()]\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"BEST/WORST PERFORMERS\")\n",
        "print(f\"Best (lowest MAE):  {best_mae['function']} - MAE={best_mae['mae']:.2f}\")\n",
        "print(f\"Worst (highest MAE): {worst_mae['function']} - MAE={worst_mae['mae']:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anY0WIbhXpps"
      },
      "outputs": [],
      "source": [
        "# Map the exported function names back to original data\n",
        "function_mapping = {\n",
        "    'func_235': func_stats.iloc[0]['func'],  # Top function by invocation count\n",
        "    'func_242': func_stats.iloc[1]['func'],\n",
        "    'func_126': func_stats.iloc[2]['func'],\n",
        "    'func_186': func_stats.iloc[4]['func'],\n",
        "    'func_190': func_stats.iloc[6]['func'],\n",
        "    'func_110': func_stats.iloc[7]['func'],\n",
        "    'func_99': func_stats.iloc[8]['func'],\n",
        "    'func_191': func_stats.iloc[9]['func']\n",
        "}\n",
        "\n",
        "# Create detailed profiles\n",
        "detailed_profiles = []\n",
        "\n",
        "for i in range(8):  # Excluding the 2 problematic ones\n",
        "    func_name = f'func_{i+1}'\n",
        "    original_func_id = func_stats.iloc[i]['func']\n",
        "    app_id = func_stats.iloc[i]['app']\n",
        "\n",
        "    # Get function data\n",
        "    func_data = df_cp[(df_cp['app'] == app_id) & (df_cp['func'] == original_func_id)]\n",
        "\n",
        "    if i < len(FUNCTION_IDS):\n",
        "        func_name = FUNCTION_IDS[i] # Use the descriptive name\n",
        "    else:\n",
        "        func_name = f'func_{i+1}' # Fallback\n",
        "    # ----------------------\n",
        "\n",
        "    original_func_id = func_stats.iloc[i]['func']\n",
        "    app_id = func_stats.iloc[i]['app']\n",
        "\n",
        "    # Calculate detailed statistics\n",
        "    profile = {\n",
        "        'display_name': func_name,\n",
        "        'original_id': original_func_id[:20] + '...',  # Truncated hash\n",
        "        'app_id': app_id[:20] + '...',\n",
        "\n",
        "        # Volume metrics\n",
        "        'total_invocations': len(func_data),\n",
        "        'invocations_per_hour': len(func_data) / func_stats.iloc[i]['activity_span_hours'],\n",
        "\n",
        "        # Traffic density\n",
        "        'zero_rate': (func_data['duration'] == 0).sum() / len(func_data) * 100,\n",
        "        'non_zero_rate': 100 - ((func_data['duration'] == 0).sum() / len(func_data) * 100),\n",
        "\n",
        "        # Performance characteristics\n",
        "        'avg_duration_ms': func_data['duration'].mean() * 1000,\n",
        "        'p95_duration_ms': func_data['duration'].quantile(0.95) * 1000,\n",
        "        'duration_variability': func_data['duration'].std() / func_data['duration'].mean(),\n",
        "\n",
        "        # Cold start analysis\n",
        "        'cold_start_rate': func_stats.iloc[i]['cold_start_rate'] * 100,\n",
        "        'total_cold_starts': func_stats.iloc[i]['cold_starts'],\n",
        "\n",
        "        # Temporal patterns\n",
        "        'activity_span_hours': func_stats.iloc[i]['activity_span_hours'],\n",
        "        'peak_hour': func_data.groupby('hour').size().idxmax(),\n",
        "        'peak_day': ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'][\n",
        "            func_data.groupby('dayofweek').size().idxmax()\n",
        "        ],\n",
        "\n",
        "        # Burstiness metrics\n",
        "        'median_time_gap_sec': func_data['time_gap'].median(),\n",
        "        'mean_time_gap_sec': func_data['time_gap'].mean(),\n",
        "        'burst_invocations': ((func_data['time_gap'] <= 10) & func_data['time_gap'].notna()).sum(),\n",
        "        'burst_rate': ((func_data['time_gap'] <= 10) & func_data['time_gap'].notna()).sum() / len(func_data) * 100,\n",
        "\n",
        "        # Workload classification\n",
        "        'workload_type': active_funcs[active_funcs['func'] == original_func_id]['workload_type'].values[0]\n",
        "            if original_func_id in active_funcs['func'].values else 'Unknown'\n",
        "    }\n",
        "\n",
        "    detailed_profiles.append(profile)\n",
        "\n",
        "# Convert to DataFrame for easy viewing\n",
        "profiles_df = pd.DataFrame(detailed_profiles)\n",
        "\n",
        "# Display\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"DETAILED FUNCTION PROFILES\")\n",
        "print(\"=\"*100)\n",
        "print(profiles_df.to_string(index=False))\n",
        "\n",
        "# Export\n",
        "profiles_df.to_csv('function_detailed_profiles.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdkKtmOiWpn"
      },
      "source": [
        "Hybrid LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJdI1o7JWI3"
      },
      "outputs": [],
      "source": [
        "# LSTM Hyperparameters\n",
        "LSTM_LOOKBACK = 30 # Use last 30 timesteps to predict next\n",
        "LSTM_UNITS = 50 # memory cells in each LSTM layer\n",
        "LSTM_LAYERS = 2 # Number of LSTM layers\n",
        "DROPOUT_RATE = 0.2 # Dropout for regularization\n",
        "EPOCHS = 100  #  Maximum number of times to train on the entire dataset\n",
        "BATCH_SIZE = 32  #  Number of samples processed before updating model weights\n",
        "VALIDATION_SPLIT = 0.2  #  20% of training data reserved for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apJ0gmkcK9Fm"
      },
      "outputs": [],
      "source": [
        "#Create sequences for LSTM training\n",
        "def create_sequences(data, lookback):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:i + lookback])\n",
        "        y.append(data[i + lookback])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGM0LV-XLpps"
      },
      "outputs": [],
      "source": [
        "#Build LSTM model architecture\n",
        "def build_lstm_model(lookback, units=50, layers=2, dropout=0.2):\n",
        "    model = Sequential() #1-- create a model\n",
        "\n",
        "    #2-- First LSTM layer\n",
        "    model.add(LSTM(\n",
        "        units=units,\n",
        "        return_sequences=(layers > 1), #true\n",
        "        input_shape=(lookback, 1)\n",
        "    ))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    # Additional LSTM layers\n",
        "    for i in range(1, layers):\n",
        "        model.add(LSTM(            #3-- second lstm layer\n",
        "            units=units,\n",
        "            return_sequences=(i < layers - 1) #false\n",
        "        ))\n",
        "        model.add(Dropout(dropout))   #4-- dropping some units to avoid mistakes\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1))     #5-- dense layer-for decision making, what to do with the processing, 1-because only 1 output number to predict\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae']) #compile to tell the model how to improve\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfhcM3KBLxDX"
      },
      "outputs": [],
      "source": [
        "#Calculate MAPE, handling zero values\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyQCJ0KMdIE"
      },
      "source": [
        "Train LSTM on Prophet residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiYfarfLMCrX"
      },
      "outputs": [],
      "source": [
        "def train_lstm_on_residuals(df, func_id, prophet_result):  #returns dict with LSTM model and metrics\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Training LSTM for {func_id}\")\n",
        "    print(f\"\\n\")\n",
        "\n",
        "    # Getting Prophet's test predictions\n",
        "    test_data = prophet_result['test']\n",
        "    forecast = prophet_result['forecast']\n",
        "\n",
        "    # Aligning forecast with test data\n",
        "    forecast_aligned = forecast.set_index('ds').loc[test_data['ds']]\n",
        "\n",
        "    # Calculating residuals (Prophet's errors)\n",
        "    residuals = test_data['y'].values - forecast_aligned['yhat'].values\n",
        "\n",
        "    print(f\"Residual statistics:\")\n",
        "    print(f\"  Mean: {residuals.mean():.2f}\")\n",
        "    print(f\"  Std:  {residuals.std():.2f}\")\n",
        "    print(f\"  Min:  {residuals.min():.2f}\")\n",
        "    print(f\"  Max:  {residuals.max():.2f}\")\n",
        "\n",
        "    # Checking if we have enough data\n",
        "    if len(residuals) < LSTM_LOOKBACK + 50:  # Need minimum data for LSTM\n",
        "        print(f\"  Insufficient data for LSTM (need >{LSTM_LOOKBACK + 50}, have {len(residuals)})\")\n",
        "        return None\n",
        "\n",
        "    # Create sequences for LSTM\n",
        "    X, y = create_sequences(residuals, LSTM_LOOKBACK)\n",
        "\n",
        "    print(f\"LSTM sequences created: {len(X)} samples\")\n",
        "\n",
        "    # Split into train and validation\n",
        "    split_idx = int(len(X) * (1 - VALIDATION_SPLIT))\n",
        "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    # Scale the data\n",
        "    X_scaler = MinMaxScaler()\n",
        "    y_scaler = MinMaxScaler()\n",
        "\n",
        "    X_train_scaled = X_scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape[0], LSTM_LOOKBACK, 1)\n",
        "    X_val_scaled = X_scaler.transform(X_val.reshape(-1, 1)).reshape(X_val.shape[0], LSTM_LOOKBACK, 1)\n",
        "\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1))\n",
        "\n",
        "    # Build and train LSTM\n",
        "    lstm_model = build_lstm_model(\n",
        "        lookback=LSTM_LOOKBACK,\n",
        "        units=LSTM_UNITS,\n",
        "        layers=LSTM_LAYERS,\n",
        "        dropout=DROPOUT_RATE\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(\"Training LSTM...\")\n",
        "    history = lstm_model.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        validation_data=(X_val_scaled, y_val_scaled),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(f\" Training completed:\")\n",
        "    print(f\"  Epochs: {len(history.history['loss'])}\")\n",
        "    print(f\"  Final train loss: {history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"  Final val loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    # Predicting residuals on validation set\n",
        "    y_pred_scaled = lstm_model.predict(X_val_scaled, verbose=0)\n",
        "    y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "    # Calculating LSTM residual prediction accuracy\n",
        "    lstm_mae = mean_absolute_error(y_val, y_pred)\n",
        "    lstm_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    print(f\"LSTM residual prediction:\")\n",
        "    print(f\"  MAE:  {lstm_mae:.2f}\")\n",
        "    print(f\"  RMSE: {lstm_rmse:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'lstm_model': lstm_model,\n",
        "        'X_scaler': X_scaler,\n",
        "        'y_scaler': y_scaler,\n",
        "        'history': history,\n",
        "        'lstm_mae': lstm_mae,\n",
        "        'lstm_rmse': lstm_rmse,\n",
        "        'residuals': residuals,\n",
        "        'X_val': X_val,\n",
        "        'y_val': y_val,\n",
        "        'y_pred': y_pred\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZqfng87NczM"
      },
      "source": [
        "Evaluating Hybrid results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTr6xGo2Ngtq"
      },
      "outputs": [],
      "source": [
        "def evaluate_hybrid_model(prophet_result, lstm_result, func_id):\n",
        "    if lstm_result is None:\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Evaluating Hybrid Model for {func_id}\")\n",
        "    print(f\"\\n\")\n",
        "\n",
        "    # Get test data and Prophet predictions\n",
        "    test_data = prophet_result['test']\n",
        "    forecast = prophet_result['forecast']\n",
        "    forecast_aligned = forecast.set_index('ds').loc[test_data['ds']]\n",
        "\n",
        "    prophet_pred = forecast_aligned['yhat'].values\n",
        "    actual = test_data['y'].values\n",
        "\n",
        "    # Get LSTM residual predictions\n",
        "    residuals = lstm_result['residuals']\n",
        "    X_val = lstm_result['X_val']\n",
        "\n",
        "    # Predict residuals with LSTM for the entire test set\n",
        "    X_full, _ = create_sequences(residuals, LSTM_LOOKBACK)\n",
        "    X_full_scaled = lstm_result['X_scaler'].transform(X_full.reshape(-1, 1)).reshape(X_full.shape[0], LSTM_LOOKBACK, 1)\n",
        "\n",
        "    lstm_residual_pred_scaled = lstm_result['lstm_model'].predict(X_full_scaled, verbose=0)\n",
        "    lstm_residual_pred = lstm_result['y_scaler'].inverse_transform(lstm_residual_pred_scaled).flatten()\n",
        "\n",
        "    # Align predictions (LSTM starts after lookback period)\n",
        "    prophet_pred_aligned = prophet_pred[LSTM_LOOKBACK:]\n",
        "    actual_aligned = actual[LSTM_LOOKBACK:]\n",
        "\n",
        "    # Create hybrid predictions\n",
        "    hybrid_pred = prophet_pred_aligned + lstm_residual_pred\n",
        "    hybrid_pred = np.maximum(hybrid_pred, 0)  # No negative predictions\n",
        "\n",
        "    # Calculate metrics for Prophet-only\n",
        "    prophet_mae = mean_absolute_error(actual_aligned, prophet_pred_aligned)\n",
        "    prophet_rmse = np.sqrt(mean_squared_error(actual_aligned, prophet_pred_aligned))\n",
        "    prophet_mape = calculate_mape(actual_aligned, prophet_pred_aligned)\n",
        "\n",
        "    # Calculate metrics for Hybrid\n",
        "    hybrid_mae = mean_absolute_error(actual_aligned, hybrid_pred)\n",
        "    hybrid_rmse = np.sqrt(mean_squared_error(actual_aligned, hybrid_pred))\n",
        "    hybrid_mape = calculate_mape(actual_aligned, hybrid_pred)\n",
        "\n",
        "    # Calculate improvement\n",
        "    mae_improvement = ((prophet_mae - hybrid_mae) / prophet_mae) * 100\n",
        "    rmse_improvement = ((prophet_rmse - hybrid_rmse) / prophet_rmse) * 100\n",
        "\n",
        "    print(f\"\\nProphet-Only Performance:\")\n",
        "    print(f\"  MAE:  {prophet_mae:.2f}\")\n",
        "    print(f\"  RMSE: {prophet_rmse:.2f}\")\n",
        "    print(f\"  MAPE: {prophet_mape:.2f}%\")\n",
        "\n",
        "    print(f\"\\nHybrid Model Performance:\")\n",
        "    print(f\"  MAE:  {hybrid_mae:.2f}\")\n",
        "    print(f\"  RMSE: {hybrid_rmse:.2f}\")\n",
        "    print(f\"  MAPE: {hybrid_mape:.2f}%\")\n",
        "\n",
        "    print(f\"\\nImprovement:\")\n",
        "    print(f\"  MAE:  {mae_improvement:+.2f}%\")\n",
        "    print(f\"  RMSE: {rmse_improvement:+.2f}%\")\n",
        "\n",
        "    if mae_improvement > 0:\n",
        "        print(f\"\\n Hybrid model improved by {mae_improvement:.1f}%!\")\n",
        "    else:\n",
        "        print(f\"\\n  Prophet alone performs better (degradation: {mae_improvement:.1f}%)\")\n",
        "\n",
        "    return {\n",
        "        'function': func_id,\n",
        "        'prophet_mae': prophet_mae,\n",
        "        'prophet_rmse': prophet_rmse,\n",
        "        'prophet_mape': prophet_mape,\n",
        "        'hybrid_mae': hybrid_mae,\n",
        "        'hybrid_rmse': hybrid_rmse,\n",
        "        'hybrid_mape': hybrid_mape,\n",
        "        'mae_improvement': mae_improvement,\n",
        "        'rmse_improvement': rmse_improvement,\n",
        "        'prophet_pred': prophet_pred_aligned,\n",
        "        'hybrid_pred': hybrid_pred,\n",
        "        'actual': actual_aligned,\n",
        "        'test_dates': test_data['ds'].values[LSTM_LOOKBACK:]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0CkrKMQN1cB"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYLgGsONNtuS"
      },
      "outputs": [],
      "source": [
        "#Plot comparison of Prophet vs Hybrid predictions\n",
        "def plot_hybrid_comparison(eval_result, func_id, save_path=None):\n",
        "    if eval_result is None:\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "    dates = pd.to_datetime(eval_result['test_dates'])\n",
        "    actual = eval_result['actual']\n",
        "    prophet_pred = eval_result['prophet_pred']\n",
        "    hybrid_pred = eval_result['hybrid_pred']\n",
        "\n",
        "    # Plot 1: Predictions comparison\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(dates, actual, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "    ax1.plot(dates, prophet_pred, label='Prophet Only', color='blue', linewidth=1.5, alpha=0.7)\n",
        "    ax1.plot(dates, hybrid_pred, label='Hybrid (Prophet+LSTM)', color='red', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "    ax1.set_xlabel('Time', fontsize=12)\n",
        "    ax1.set_ylabel('Requests per Minute', fontsize=12)\n",
        "    ax1.set_title(f'{func_id}: Hybrid Model Predictions', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='upper left', fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Residuals comparison\n",
        "    ax2 = axes[1]\n",
        "    prophet_residuals = actual - prophet_pred\n",
        "    hybrid_residuals = actual - hybrid_pred\n",
        "\n",
        "    ax2.plot(dates, prophet_residuals, label='Prophet Residuals', color='blue', alpha=0.6)\n",
        "    ax2.plot(dates, hybrid_residuals, label='Hybrid Residuals', color='red', alpha=0.6)\n",
        "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    ax2.set_xlabel('Time', fontsize=12)\n",
        "    ax2.set_ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "    ax2.set_title('Prediction Residuals', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(loc='upper left', fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add improvement text\n",
        "    improvement_text = f\"MAE Improvement: {eval_result['mae_improvement']:+.1f}%\"\n",
        "    ax1.text(0.02, 0.98, improvement_text,\n",
        "             transform=ax1.transAxes,\n",
        "             fontsize=11,\n",
        "             verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\" Plot saved: {save_path}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_nxKTbkOFo6"
      },
      "source": [
        "Batch processing by hybrid model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJMo1dudOFMg"
      },
      "outputs": [],
      "source": [
        "def process_all_functions(results): #existing prophet results\n",
        "    print(\"\\n\")\n",
        "    print(\" HYBRID PROPHET-LSTM BATCH TRAINING\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    hybrid_results = []\n",
        "    lstm_models = {}\n",
        "\n",
        "    for prophet_result in tqdm(results, desc=\"Training LSTM models\"):\n",
        "        func_id = prophet_result['function']\n",
        "\n",
        "        try:\n",
        "            # Load original time series data\n",
        "            df = pd.read_csv(f'timeseries_{func_id}.csv')\n",
        "            df['ds'] = pd.to_datetime(df['ds'])\n",
        "            df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
        "            df = df.dropna(subset=['ds', 'y'])\n",
        "\n",
        "            # Train LSTM on residuals\n",
        "            lstm_result = train_lstm_on_residuals(df, func_id, prophet_result)\n",
        "\n",
        "            if lstm_result is None:\n",
        "                print(f\"  Skipping {func_id} - insufficient data\")\n",
        "                continue\n",
        "\n",
        "            # Evaluate hybrid model\n",
        "            eval_result = evaluate_hybrid_model(prophet_result, lstm_result, func_id)\n",
        "\n",
        "            if eval_result is not None:\n",
        "                hybrid_results.append(eval_result)\n",
        "                lstm_models[func_id] = lstm_result\n",
        "\n",
        "                # Plot comparison\n",
        "                plot_hybrid_comparison(eval_result, func_id,\n",
        "                                     save_path=f'hybrid_{func_id}_prediction.png')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error processing {func_id}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return hybrid_results, lstm_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFY8E0_MN_ba"
      },
      "outputs": [],
      "source": [
        "#Summary report\n",
        "def generate_summary_report(hybrid_results):\n",
        "    if len(hybrid_results) == 0:\n",
        "        print(\"No results to summarize.\")\n",
        "        return\n",
        "\n",
        "    # Create DataFrame\n",
        "    summary_df = pd.DataFrame(hybrid_results)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\" HYBRID MODEL SUMMARY REPORT\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Display results table\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    print(summary_df[['function', 'prophet_mae', 'hybrid_mae', 'mae_improvement',\n",
        "                      'prophet_rmse', 'hybrid_rmse', 'rmse_improvement']].to_string(index=False))\n",
        "\n",
        "    # Overall statistics\n",
        "    print(\"\\n\")\n",
        "    print(\"Overall Performance:\")\n",
        "    print(\"\\n\")\n",
        "    print(f\"\\nProphet Average:\")\n",
        "    print(f\"  MAE:  {summary_df['prophet_mae'].mean():.2f} (±{summary_df['prophet_mae'].std():.2f})\")\n",
        "    print(f\"  RMSE: {summary_df['prophet_rmse'].mean():.2f} (±{summary_df['prophet_rmse'].std():.2f})\")\n",
        "    print(f\"  MAPE: {summary_df['prophet_mape'].mean():.2f}%\")\n",
        "\n",
        "    print(f\"\\nHybrid Average:\")\n",
        "    print(f\"  MAE:  {summary_df['hybrid_mae'].mean():.2f} (±{summary_df['hybrid_mae'].std():.2f})\")\n",
        "    print(f\"  RMSE: {summary_df['hybrid_rmse'].mean():.2f} (±{summary_df['hybrid_rmse'].std():.2f})\")\n",
        "    print(f\"  MAPE: {summary_df['hybrid_mape'].mean():.2f}%\")\n",
        "\n",
        "    print(f\"\\nAverage Improvement:\")\n",
        "    print(f\"  MAE:  {summary_df['mae_improvement'].mean():+.2f}%\")\n",
        "    print(f\"  RMSE: {summary_df['rmse_improvement'].mean():+.2f}%\")\n",
        "\n",
        "    # Count improvements\n",
        "    improved = (summary_df['mae_improvement'] > 0).sum()\n",
        "    total = len(summary_df)\n",
        "\n",
        "    print(f\"\\nFunctions Improved: {improved}/{total} ({improved/total*100:.1f}%)\")\n",
        "\n",
        "    # Save results\n",
        "    summary_df.to_csv('hybrid_model_results.csv', index=False)\n",
        "    print(f\"\\n Results saved to: hybrid_model_results.csv\")\n",
        "\n",
        "    # Create comparison visualization\n",
        "    create_summary_plots(summary_df)\n",
        "\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeAVZBK9O5RO"
      },
      "outputs": [],
      "source": [
        "#Create summary comparison plots\n",
        "def create_summary_plots(summary_df):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    x = np.arange(len(summary_df))\n",
        "    width = 0.35\n",
        "\n",
        "    # Plot 1: MAE Comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.bar(x - width/2, summary_df['prophet_mae'], width,\n",
        "            label='Prophet', color='blue', alpha=0.7)\n",
        "    ax1.bar(x + width/2, summary_df['hybrid_mae'], width,\n",
        "            label='Hybrid', color='red', alpha=0.7)\n",
        "    ax1.set_xlabel('Function')\n",
        "    ax1.set_ylabel('MAE')\n",
        "    ax1.set_title('MAE: Prophet vs Hybrid', fontweight='bold')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels([f.replace('func_', 'F') for f in summary_df['function']], rotation=45)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 2: RMSE Comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.bar(x - width/2, summary_df['prophet_rmse'], width,\n",
        "            label='Prophet', color='blue', alpha=0.7)\n",
        "    ax2.bar(x + width/2, summary_df['hybrid_rmse'], width,\n",
        "            label='Hybrid', color='red', alpha=0.7)\n",
        "    ax2.set_xlabel('Function')\n",
        "    ax2.set_ylabel('RMSE')\n",
        "    ax2.set_title('RMSE: Prophet vs Hybrid', fontweight='bold')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels([f.replace('func_', 'F') for f in summary_df['function']], rotation=45)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 3: Improvement Percentage\n",
        "    ax3 = axes[1, 0]\n",
        "    colors = ['green' if val > 0 else 'red' for val in summary_df['mae_improvement']]\n",
        "    ax3.bar(x, summary_df['mae_improvement'], color=colors, alpha=0.7)\n",
        "    ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "    ax3.set_xlabel('Function')\n",
        "    ax3.set_ylabel('MAE Improvement (%)')\n",
        "    ax3.set_title('Hybrid Improvement over Prophet', fontweight='bold')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels([f.replace('func_', 'F') for f in summary_df['function']], rotation=45)\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 4: Scatter\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.scatter(summary_df['prophet_mae'], summary_df['hybrid_mae'],\n",
        "                s=100, alpha=0.6, c='purple')\n",
        "    max_val = max(summary_df['prophet_mae'].max(), summary_df['hybrid_mae'].max())\n",
        "    ax4.plot([0, max_val], [0, max_val], 'k--', linewidth=1, alpha=0.5)\n",
        "    ax4.set_xlabel('Prophet MAE')\n",
        "    ax4.set_ylabel('Hybrid MAE')\n",
        "    ax4.set_title('Prophet vs Hybrid MAE\\n(Below diagonal = Hybrid better)', fontweight='bold')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    improved = (summary_df['hybrid_mae'] < summary_df['prophet_mae']).sum()\n",
        "    ax4.text(0.05, 0.95, f'{improved}/{len(summary_df)} improved',\n",
        "             transform=ax4.transAxes,\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hybrid_summary_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    print(\" Summary plots saved: hybrid_summary_comparison.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3O9dimVPKSg"
      },
      "outputs": [],
      "source": [
        "print(\" Starting Hybrid LSTM Training...\")\n",
        "if 'results' not in locals():\n",
        "    print(\" ERROR: 'results' variable not found\")\n",
        "    print(\"Run Prophet training code first\")\n",
        "else:\n",
        "    print(f\" Found {len(results)} Prophet results\")\n",
        "    print(\"\\nProcessing functions with LSTM...\\n\")\n",
        "\n",
        "    # Train LSTM on all functions\n",
        "    hybrid_results, lstm_models = process_all_functions(results)\n",
        "\n",
        "    # Generate summary report\n",
        "    if len(hybrid_results) > 0:\n",
        "        print(\"\\n\" )\n",
        "        print(\"Generating Summary Report...\")\n",
        "        summary_df = generate_summary_report(hybrid_results)\n",
        "\n",
        "        print(\"\\n DONE! \")\n",
        "    else:\n",
        "        print(\"\\n No hybrid results generated. Check errors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTOYkZxMQe7U"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "'''# Testing lookback values\n",
        "lookback_values = [15, 30, 45, 60]\n",
        "\n",
        "for LSTM_LOOKBACK in lookback_values:\n",
        "    print(f\"\\nTesting LSTM_LOOKBACK = {LSTM_LOOKBACK}\")\n",
        "\n",
        "    func_id = 'func_110'\n",
        "    df = pd.read_csv(f'timeseries_{func_id}.csv')\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df['y'] = pd.to_numeric(df['y'])\n",
        "    df = df.dropna()\n",
        "\n",
        "    prophet_result = [r for r in results if r['function'] == func_id][0]\n",
        "\n",
        "    # This uses your existing function with the updated LSTM_LOOKBACK\n",
        "    lstm_result = train_lstm_on_residuals(df, func_id, prophet_result)\n",
        "\n",
        "    if lstm_result:\n",
        "        eval_result = evaluate_hybrid_model(prophet_result, lstm_result, func_id)\n",
        "        if eval_result:\n",
        "            print(f\"\\n Lookback {LSTM_LOOKBACK}: MAE = {eval_result['hybrid_mae']:.2f}, \"\n",
        "                  f\"Improvement = {eval_result['mae_improvement']:+.1f}%\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rr_-l20K09B"
      },
      "outputs": [],
      "source": [
        "# Test lookback values\n",
        "lookback_values = [15, 30, 45, 60]\n",
        "\n",
        "for LSTM_LOOKBACK in lookback_values:\n",
        "    print(f\"\\nTesting LSTM_LOOKBACK = {LSTM_LOOKBACK}\")\n",
        "\n",
        "    func_id = 'func_235'\n",
        "    df = pd.read_csv(f'timeseries_{func_id}.csv')\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df['y'] = pd.to_numeric(df['y'])\n",
        "    df = df.dropna()\n",
        "\n",
        "    prophet_result = [r for r in results if r['function'] == func_id][0]\n",
        "\n",
        "    # This uses your existing function with the updated LSTM_LOOKBACK\n",
        "    lstm_result = train_lstm_on_residuals(df, func_id, prophet_result)\n",
        "\n",
        "    if lstm_result:\n",
        "        eval_result = evaluate_hybrid_model(prophet_result, lstm_result, func_id)\n",
        "        if eval_result:\n",
        "            print(f\"\\n Lookback {LSTM_LOOKBACK}: MAE = {eval_result['hybrid_mae']:.2f}, \"\n",
        "                  f\"Improvement = {eval_result['mae_improvement']:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2jjGEWd0M4z"
      },
      "source": [
        "LSTM only model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-3TjqkhK5Nt"
      },
      "outputs": [],
      "source": [
        "# Use same configuration as hybrid\n",
        "LSTM_LOOKBACK = 30\n",
        "LSTM_UNITS = 50\n",
        "LSTM_LAYERS = 2\n",
        "DROPOUT_RATE = 0.2\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lna3dgFi0X0e"
      },
      "outputs": [],
      "source": [
        "def train_lstm_only(df, func_id):\n",
        "  print(f\"Training LSTM-only for {func_id}\")\n",
        "  #prepare data\n",
        "  df = df.copy()\n",
        "  df['ds'] = pd.to_datetime(df['ds'])\n",
        "  df['y'] = pd.to_numeric(df['y'], errors = 'coerce')\n",
        "  df = df.dropna(subset = ['ds','y'])\n",
        "\n",
        "  print(f\"Total data points = {len(df):,}\")\n",
        "  train_size = int(len(df)*0.8)\n",
        "  train_df = df[:train_size]\n",
        "  test_df = df[train_size:]\n",
        "\n",
        "  print(f\"Train size: {len(train_df):,}\")\n",
        "  print(f\"Test size: {len(test_df):,}\")\n",
        "\n",
        "  #Extract values\n",
        "  train_data = train_df['y']\n",
        "  test_data = test_df['y']\n",
        "\n",
        "  #create sequences\n",
        "  print(f\"\\nCreating sequence with lookback value: {LSTM_LOOKBACK}...\")\n",
        "\n",
        "  X_train, y_train = create_sequences(train_data, LSTM_LOOKBACK)\n",
        "\n",
        "  if len(X_train) < 50:\n",
        "    print(f\"Insufficient training data only {len(X_train)} samples\")\n",
        "    return None\n",
        "\n",
        "  print(f\"Training sequences {len(X_train):,}\")\n",
        "\n",
        "  #Split train into train and validation\n",
        "\n",
        "  split_idx = int(len(X_train)* (1-VALIDATION_SPLIT))\n",
        "  X_train_final = X_train[:split_idx]\n",
        "  x_val = X_train[split_idx:]\n",
        "  y_train_final = y_train[:split_idx]\n",
        "  y_val = y_train[split_idx:]\n",
        "\n",
        "  #Scale the data\n",
        "  X_scaler = MinMaxScaler()\n",
        "  y_scaler = MinMaxScaler()\n",
        "\n",
        "  X_train_scaled = X_scaler.fit_transform(X_train_final.reshape(-1,1)).reshape(X_train_final.shape[0], LSTM_LOOKBACK, 1)\n",
        "  x_val_scaled = X_scaler.transform(x_val.reshape(-1,1)).reshape(x_val.shape[0], LSTM_LOOKBACK, 1)\n",
        "\n",
        "  y_train_scaled = y_scaler.fit_transform(y_train_final.reshape(-1,1))\n",
        "  y_val_scaled = y_scaler.transform(y_val.reshape(-1,1))\n",
        "\n",
        "  #Build LSTM model\n",
        "  print(\"\\nBuilding LSTM model\")\n",
        "  print(f\"LSTM Architecture: {LSTM_LAYERS} layers, {LSTM_UNITS} units, {DROPOUT_RATE} dropout\")\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  #Adding first layer\n",
        "  model.add(LSTM(\n",
        "      units = LSTM_UNITS,\n",
        "      return_sequences = (LSTM_LAYERS>1),\n",
        "      input_shape = (LSTM_LOOKBACK,1)\n",
        "  ))\n",
        "\n",
        "  model.add(Dropout(DROPOUT_RATE))\n",
        "\n",
        "  #Additional LSTM layers\n",
        "\n",
        "  for i in range(1, LSTM_LAYERS):\n",
        "    model.add(LSTM(\n",
        "        units = LSTM_UNITS,\n",
        "        return_sequences = (i < LSTM_LAYERS - 1)\n",
        "    ))\n",
        "\n",
        "  model.add(Dropout(DROPOUT_RATE))\n",
        "\n",
        "  #output layer\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss = 'mse', metrics = ['mae'])\n",
        "\n",
        "  #Train\n",
        "  print(\"Training LSTM...\")\n",
        "\n",
        "  early_stopping = EarlyStopping(\n",
        "      monitor = 'val_loss',\n",
        "      patience = 10,\n",
        "      restore_best_weights = True,\n",
        "      verbose = 0\n",
        "  )\n",
        "\n",
        "  history = model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data = (x_val_scaled, y_val_scaled),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    callbacks = [early_stopping],\n",
        "    verbose = 0\n",
        "  )\n",
        "\n",
        "  print(f\" Training completed:\")\n",
        "  print(f\"  Epochs: {len(history.history['loss'])}\")\n",
        "  print(f\"  Final train loss: {history.history['loss'][-1]:.4f}\")\n",
        "  print(f\"  Final val loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "  # Prepare test sequences\n",
        "  print(f\"\\nPreparing test predictions...\")\n",
        "\n",
        "  # Use last LSTM_LOOKBACK points from training as initial context\n",
        "  test_data_with_context = np.concatenate([train_data[-LSTM_LOOKBACK:], test_data])\n",
        "\n",
        "  # Create test sequences\n",
        "  X_test, y_test = create_sequences(test_data_with_context, LSTM_LOOKBACK)\n",
        "\n",
        "  # The first len(test_data) predictions correspond to our test set\n",
        "  X_test = X_test[:len(test_data)]\n",
        "  y_test = y_test[:len(test_data)]\n",
        "\n",
        "  # Scale test data\n",
        "  X_test_scaled = X_scaler.transform(X_test.reshape(-1, 1)).reshape(\n",
        "    X_test.shape[0], LSTM_LOOKBACK, 1)\n",
        "\n",
        "  # Predict\n",
        "  y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
        "  y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "  # Ensure non-negative predictions\n",
        "  y_pred = np.maximum(y_pred, 0)\n",
        "\n",
        "  # Calculate metrics\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "  # Calculate MAPE\n",
        "  mask = y_test != 0\n",
        "  if mask.sum() > 0:\n",
        "    mape = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100\n",
        "  else:\n",
        "     mape = np.nan\n",
        "\n",
        "  print(f\"\\nLSTM-Only Test Performance:\")\n",
        "  print(f\"  MAE:  {mae:.2f}\")\n",
        "  print(f\"  RMSE: {rmse:.2f}\")\n",
        "  print(f\"  MAPE: {mape:.2f}%\")\n",
        "\n",
        "  return {\n",
        "        'model': model,\n",
        "        'X_scaler': X_scaler,\n",
        "        'y_scaler': y_scaler,\n",
        "        'history': history,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'mape': mape,\n",
        "        'y_test': y_test,\n",
        "        'y_pred': y_pred,\n",
        "        'test_dates': test_df['ds'].values[:len(y_pred)]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tMd_G_gibnu"
      },
      "outputs": [],
      "source": [
        "def compare_all_models(func_id, prophet_result, lstm_only_result, hybrid_result):\n",
        "  print(f\"THREE-WAY MODEL COMPARISON: {func_id}\")\n",
        "\n",
        "  # Get Prophet metrics\n",
        "  test_data = prophet_result['test']\n",
        "  forecast = prophet_result['forecast']\n",
        "  prophet_pred = forecast['yhat'].values\n",
        "  actual = test_data['y'].values\n",
        "\n",
        "  prophet_mae = mean_absolute_error(actual, prophet_pred)\n",
        "  prophet_rmse = np.sqrt(mean_squared_error(actual, prophet_pred))\n",
        "\n",
        "  # LSTM-only metrics (already calculated)\n",
        "  lstm_mae = lstm_only_result['mae']\n",
        "  lstm_rmse = lstm_only_result['rmse']\n",
        "\n",
        "  # Hybrid metrics (already calculated)\n",
        "  hybrid_mae = hybrid_result['hybrid_mae']\n",
        "  hybrid_rmse = hybrid_result['hybrid_rmse']\n",
        "\n",
        "  print(\"\\n PERFORMANCE COMPARISON\")\n",
        "\n",
        "  print(f\"{'Model':<20} {'MAE':<12} {'RMSE':<12} {'vs Prophet':<15}\")\n",
        "\n",
        "  print(f\"{'Prophet-only':<20} {prophet_mae:<12.2f} {prophet_rmse:<12.2f} {'(baseline)':<15}\")\n",
        "\n",
        "  lstm_vs_prophet = ((prophet_mae - lstm_mae) / prophet_mae) * 100\n",
        "  print(f\"{'LSTM-only':<20} {lstm_mae:<12.2f} {lstm_rmse:<12.2f} {lstm_vs_prophet:+.1f}%\")\n",
        "\n",
        "  hybrid_vs_prophet = ((prophet_mae - hybrid_mae) / prophet_mae) * 100\n",
        "  print(f\"{'Hybrid (P+L)':<20} {hybrid_mae:<12.2f} {hybrid_rmse:<12.2f} {hybrid_vs_prophet:+.1f}%\")\n",
        "\n",
        "\n",
        "  # Determine best model\n",
        "  best_mae = min(prophet_mae, lstm_mae, hybrid_mae)\n",
        "\n",
        "  if best_mae == prophet_mae:\n",
        "      best_model = \"Prophet-only\"\n",
        "  elif best_mae == lstm_mae:\n",
        "      best_model = \"LSTM-only\"\n",
        "  else:\n",
        "      best_model = \"Hybrid\"\n",
        "\n",
        "  print(f\"\\ Best Model: {best_model} (MAE: {best_mae:.2f})\")\n",
        "\n",
        "  # Additional comparisons\n",
        "  print(f\"\\n RELATIVE PERFORMANCE\")\n",
        "\n",
        "  if lstm_mae < prophet_mae:\n",
        "      print(f\" LSTM-only beats Prophet by {lstm_vs_prophet:.1f}%\")\n",
        "  else:\n",
        "      print(f\" LSTM-only worse than Prophet by {-lstm_vs_prophet:.1f}%\")\n",
        "\n",
        "  if hybrid_mae < prophet_mae:\n",
        "      print(f\" Hybrid beats Prophet by {hybrid_vs_prophet:.1f}%\")\n",
        "  else:\n",
        "      print(f\" Hybrid worse than Prophet by {-hybrid_vs_prophet:.1f}%\")\n",
        "\n",
        "  if hybrid_mae < lstm_mae:\n",
        "      lstm_vs_hybrid = ((lstm_mae - hybrid_mae) / lstm_mae) * 100\n",
        "      print(f\" Hybrid beats LSTM-only by {lstm_vs_hybrid:.1f}%\")\n",
        "  else:\n",
        "      lstm_vs_hybrid = ((lstm_mae - hybrid_mae) / lstm_mae) * 100\n",
        "      print(f\" Hybrid worse than LSTM-only by {-lstm_vs_hybrid:.1f}%\")\n",
        "\n",
        "  return {\n",
        "        'function': func_id,\n",
        "        'prophet_mae': prophet_mae,\n",
        "        'prophet_rmse': prophet_rmse,\n",
        "        'lstm_mae': lstm_mae,\n",
        "        'lstm_rmse': lstm_rmse,\n",
        "        'hybrid_mae': hybrid_mae,\n",
        "        'hybrid_rmse': hybrid_rmse,\n",
        "        'best_model': best_model,\n",
        "        'prophet_vs_lstm': lstm_vs_prophet,\n",
        "        'prophet_vs_hybrid': hybrid_vs_prophet,\n",
        "        'lstm_vs_hybrid': lstm_vs_hybrid\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgeBsjKio7So"
      },
      "outputs": [],
      "source": [
        "def plot_three_way_comparison(func_id, prophet_result, lstm_only_result, hybrid_result):\n",
        "    \"\"\"Create visualization comparing all three models.\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "    # Get data\n",
        "    test_data = prophet_result['test']\n",
        "    actual = test_data['y'].values\n",
        "    dates = test_data['ds'].values\n",
        "\n",
        "    prophet_pred = prophet_result['forecast']['yhat'].values\n",
        "\n",
        "    # LSTM predictions (need to align)\n",
        "    lstm_pred = lstm_only_result['y_pred']\n",
        "    lstm_dates = lstm_only_result['test_dates']\n",
        "\n",
        "    # Hybrid predictions (already aligned)\n",
        "    hybrid_pred = hybrid_result['hybrid_pred']\n",
        "    hybrid_dates = hybrid_result['test_dates']\n",
        "\n",
        "    # Plot 1: All predictions\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(dates, actual, label='Actual', color='black', linewidth=2, alpha=0.8)\n",
        "    ax1.plot(dates, prophet_pred, label='Prophet', color='blue', linewidth=1.5, alpha=0.7)\n",
        "    ax1.plot(lstm_dates, lstm_pred, label='LSTM-only', color='green', linewidth=1.5, alpha=0.7)\n",
        "    ax1.plot(hybrid_dates, hybrid_pred, label='Hybrid', color='red', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "    ax1.set_xlabel('Time', fontsize=12)\n",
        "    ax1.set_ylabel('Requests per Minute', fontsize=12)\n",
        "    ax1.set_title(f'{func_id}: Three-Way Model Comparison', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='upper left', fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Residuals\n",
        "    ax2 = axes[1]\n",
        "    prophet_residuals = actual - prophet_pred\n",
        "    lstm_residuals = lstm_only_result['y_test'] - lstm_pred\n",
        "    hybrid_residuals = hybrid_result['actual'] - hybrid_pred\n",
        "\n",
        "    ax2.plot(dates, prophet_residuals, label='Prophet Residuals', color='blue', alpha=0.6)\n",
        "    ax2.plot(lstm_dates, lstm_residuals, label='LSTM Residuals', color='green', alpha=0.6)\n",
        "    ax2.plot(hybrid_dates, hybrid_residuals, label='Hybrid Residuals', color='red', alpha=0.6)\n",
        "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    ax2.set_xlabel('Time', fontsize=12)\n",
        "    ax2.set_ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "    ax2.set_title('Prediction Residuals', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(loc='upper left', fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'three_way_comparison_{func_id}.png', dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n Plot saved: three_way_comparison_{func_id}.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI5hCjybpKKK"
      },
      "outputs": [],
      "source": [
        "def process_lstm_only_all_functions(results):\n",
        "    \"\"\"Train LSTM-only model on all functions.\"\"\"\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"BATCH LSTM-ONLY TRAINING\")\n",
        "\n",
        "    lstm_only_results = []\n",
        "\n",
        "    for prophet_result in results:\n",
        "        func_id = prophet_result['function']\n",
        "\n",
        "        print(f\"\\nProcessing {func_id}...\")\n",
        "\n",
        "        try:\n",
        "            # Load data\n",
        "            df = pd.read_csv(f'timeseries_{func_id}.csv')\n",
        "\n",
        "            # Train LSTM-only\n",
        "            lstm_result = train_lstm_only(df, func_id)\n",
        "\n",
        "            if lstm_result:\n",
        "                lstm_only_results.append({\n",
        "                    'function': func_id,\n",
        "                    'lstm_mae': lstm_result['mae'],\n",
        "                    'lstm_rmse': lstm_result['rmse'],\n",
        "                    'lstm_mape': lstm_result['mape'],\n",
        "                    'lstm_result': lstm_result\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error processing {func_id}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return lstm_only_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql6IdjnmpN1q"
      },
      "outputs": [],
      "source": [
        "def full_three_way_comparison(prophet_results, hybrid_results, lstm_only_results):\n",
        "    \"\"\"Create comprehensive comparison of all three approaches.\"\"\"\n",
        "\n",
        "    print(\"\\n\" )\n",
        "    print(\"COMPLETE THREE-WAY MODEL COMPARISON\")\n",
        "\n",
        "    comparison_results = []\n",
        "\n",
        "    for func_id in [r['function'] for r in prophet_results]:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Function: {func_id}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Get results for this function\n",
        "        prophet_res = [r for r in prophet_results if r['function'] == func_id][0]\n",
        "        hybrid_res = [r for r in hybrid_results if r['function'] == func_id]\n",
        "        lstm_res = [r for r in lstm_only_results if r['function'] == func_id]\n",
        "\n",
        "        if not hybrid_res or not lstm_res:\n",
        "            print(f\"  Missing results for {func_id}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        hybrid_res = hybrid_res[0]\n",
        "        lstm_res = lstm_res[0]\n",
        "\n",
        "        # Compare\n",
        "        comparison = compare_all_models(\n",
        "            func_id,\n",
        "            prophet_res,\n",
        "            lstm_res['lstm_result'],\n",
        "            hybrid_res\n",
        "        )\n",
        "\n",
        "        comparison_results.append(comparison)\n",
        "\n",
        "        # Plot\n",
        "        try:\n",
        "            plot_three_way_comparison(\n",
        "                func_id,\n",
        "                prophet_res,\n",
        "                lstm_res['lstm_result'],\n",
        "                hybrid_res\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not create plot: {str(e)}\")\n",
        "\n",
        "    # Create summary\n",
        "    comparison_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "    print(\"\\n\" )\n",
        "    print(\"OVERALL SUMMARY\")\n",
        "\n",
        "\n",
        "    print(\"\\n Average Performance:\")\n",
        "    print(f\"  Prophet MAE:  {comparison_df['prophet_mae'].mean():.2f}\")\n",
        "    print(f\"  LSTM MAE:     {comparison_df['lstm_mae'].mean():.2f}\")\n",
        "    print(f\"  Hybrid MAE:   {comparison_df['hybrid_mae'].mean():.2f}\")\n",
        "\n",
        "    print(\"\\n Best Model by Function:\")\n",
        "    best_model_counts = comparison_df['best_model'].value_counts()\n",
        "    for model, count in best_model_counts.items():\n",
        "        percentage = (count / len(comparison_df)) * 100\n",
        "        print(f\"  {model}: {count}/{len(comparison_df)} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Save results\n",
        "    comparison_df.to_csv('three_way_model_comparison.csv', index=False)\n",
        "    print(f\"\\n Results saved to: three_way_model_comparison.csv\")\n",
        "\n",
        "    return comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SucVPw_MpdhQ"
      },
      "outputs": [],
      "source": [
        "lstm_only_results = process_lstm_only_all_functions(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVobO4LIpkye"
      },
      "outputs": [],
      "source": [
        "comparison_df = full_three_way_comparison(\n",
        "    prophet_results=results,          # Prophet results\n",
        "    hybrid_results=hybrid_results,    # hybrid results\n",
        "    lstm_only_results=lstm_only_results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7sW6eHDpsl8"
      },
      "outputs": [],
      "source": [
        "print(comparison_df[['function', 'prophet_mae', 'lstm_mae', 'hybrid_mae', 'best_model']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}